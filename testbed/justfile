set dotenv-load := true

RUN := env_var_or_default("RUN", "python")
DOCKER := env_var_or_default("DOCKER", "podman")
export SSHPASS := "giraff"
export MANAGER_PATH := "../manager"
export IOT_EMULATION_PATH := "../iot_emulation"
export FUNCTIONS_PATH := "../openfaas-functions"

# Entrypoint, run deploy and then tunnels
_default:
    @just --list

init user *FLAGS:
    #!/usr/bin/env bash
    cat <<EOF > ~/.ssh/config
    Host !access.grid5000.fr *.grid5000.fr
    User {{ user }}
    ProxyJump {{ user }}@access.grid5000.fr
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
    ForwardAgent yes
    EOF

    {{ RUN }} integration.py init --g5k_user={{ user }} {{ FLAGS }}

# Open SSH tunnels
[private]
tunnels +FLAGS='':
    {{ RUN }} integration.py tunnels {{ FLAGS }}

# List end nodes of the Fog network
[private]
endpoints:
    {{ RUN }} integration.py endpoints

_is_connected city_name_node_target $market_ip market_port:
    #!/usr/bin/env bash
    set -o pipefail
    raw_output=`curl -m 30 -s "$market_ip:{{ market_port }}/api/fog"`
    output=`echo "$raw_output" | jq '.[] | select(.tags[] == "{{ city_name_node_target }}") | .id' | sed -nE 's/"(.*)"/\1/p'`
    status=$?
    if [ $status -ne 0 ]; then
        echo $raw_output
    fi
    if [ "$output" == "" ]; then
      echo $raw_output
      exit 1
    fi
    exit $status

_until_is_connected city_name_node_target market_ip market_port:
    #!/usr/bin/env bash
    status=1
    while [ $status -ne 0 ]; do
      just _is_connected {{ city_name_node_target }} {{ market_ip }} {{ market_port }}
      status=$?
      if [ $status -ne 0 ]; then
        sleep 3
      fi
    done

_until_is_connected_timeout city_name_node_target market_ip market_port timeout:
    #!/usr/bin/env bash
    output=`timeout {{ timeout }} bash -c "just _until_is_connected {{ city_name_node_target }} {{ market_ip }} {{ market_port }}" 2>&1`
    status=$?
    if [ $status -ne 0 ]; then
      echo "Failed to check connection to {{ city_name_node_target }} w/ output: $output"
    fi
    exit $status

[private]
deploy fog_node_image market_image $DEPLOYMENT_NAME:
    #!/usr/bin/env bash
    echo "REFRESH?: $REFRESH"
    if [ "$REFRESH" = "true" ] ; then
        {{ RUN }} integration.py restart
    else
        {{ RUN }} integration.py up --name="$DEPLOYMENT_NAME" --walltime "$DEPLOYMENT_WALLTIME" --force
    fi
    just _deploy {{ fog_node_image }} {{ market_image }}

[private]
refresh fog_node_image market_image $DEPLOYMENT_NAME:
    {{ RUN }} integration.py restart
    just _deploy {{ fog_node_image }} {{ market_image }}

# Delete the Job on grid'5000 and local EnosLib files
[private]
clean:
    {{ RUN }} integration.py clean || true
    rm -rf enos_* current cachedir __enos*

# Refresh the container images hosted by k3s on all deployed nodes
_deploy fog_node_image market_image:
    #!/usr/bin/env bash
    set -e
    {{ RUN }} integration.py network
    {{ RUN }} integration.py iot-emulation
    {{ RUN }} integration.py k3s-setup
    {{ RUN }} integration.py k3s-deploy --fog_node_image={{ fog_node_image }} --market_image={{ market_image }}

[private]
expe $IOT_IP $MARKET_IP $TARGET_NODES:
    {{ RUN }} expe.py

[private]
scenario archive_name +city_name_node_targets:
    #!/usr/bin/env bash
    _status=0
    trap "_status=1 _localstatus=\$?" ERR
    _localstatus=0

    NODE_TARGETS=()

    MARKET_IP=$({{ RUN }} integration.py market-ip | grep address | sed 's/address: //')
    # Create bg tasks, push their pid in an array and check after waiting for each when one has failed
    pids=()
    echo "Checking {{ city_name_node_targets }}"
    for city in {{ city_name_node_targets }}; do
        just _until_is_connected_timeout $city $MARKET_IP $MARKET_LOCAL_PORT 500&
        pids+=($!)
    done

    error_occurred=0

    for pid in "${pids[@]}"; do
      wait "$pid"
      exit_status=$?
      if [ $exit_status -ne 0 ]; then
        error_occurred=1
      fi
    done
    if [ $error_occurred -ne 0 ]; then
      echo "At least one connection failed to be checked in time. Exiting..."
      exit 1
    fi

    for city in {{ city_name_node_targets }}; do
        echo "Checking connection to $city"
        NODE_TARGETS+=($(curl "$MARKET_IP:$MARKET_LOCAL_PORT/api/fog" | jq ".[] | select(.tags[] == \"$city\") | .id" | sed -nE 's/"(.*)"/\1/p'))
    done

    if (( _localstatus > 0 )); then
        printf "Failed to get info on fog nodes (%s)" "$_localstatus"
        exit $_status
    fi
    _localstatus=0

    IOT_IP=$(just endpoints | sed -nE 's/Iot emulation IP -> (.*)/\1/p')
    printf -v joined '%s\t' "${NODE_TARGETS[@]}"
    export TARGET_NODE_NAMES="{{ city_name_node_targets }}"
    if (( _localstatus > 0 )); then
        printf "Failed to get endpoints (%s)" "$_localstatus"
        exit $_status
    fi
    _localstatus=0

    just expe $IOT_IP $MARKET_IP "$joined"
    if (( _localstatus > 0 )); then
        printf "Experiments failed (%s)" "$_localstatus"
    fi

    if (( _localstatus == 0 )); then
        sleep $WAIT_TIME

        export COLLECT_ARCHIVE_NAME="{{ archive_name }}"
        {{ RUN }} integration.py collect

        if (( _localstatus > 0 )); then
            printf "Failed to collect metrics (%s)" "$_localstatus"
            exit $_status
        fi
    fi

    sleep 60

    if [ "$DEV" = "true" ]; then
        printf "DEV activated ; waiting indefinitely\n"
        sleep 9999999999;
    fi

    exit $_status

[private]
scenarii +city_name_node_targets:
    #!/usr/bin/env bash
    set -e

    function run_scenario() {
        echo "Using image $IMAGE_REGISTRY/$COMMON_IMAGE_NAME:$2 and $IMAGE_REGISTRY/$COMMON_IMAGE_NAME:$3"
        if [ $1 -eq 1 ]; then
            just deploy "$IMAGE_REGISTRY/$COMMON_IMAGE_NAME:$2" "$IMAGE_REGISTRY/$COMMON_IMAGE_NAME:$3" $DEPLOYMENT_NAME
        else
            just refresh "$IMAGE_REGISTRY/$COMMON_IMAGE_NAME:$2" "$IMAGE_REGISTRY/$COMMON_IMAGE_NAME:$3" $DEPLOYMENT_NAME \
            || just deploy "$IMAGE_REGISTRY/$COMMON_IMAGE_NAME:$2" "$IMAGE_REGISTRY/$COMMON_IMAGE_NAME:$3" $DEPLOYMENT_NAME
        fi
        just tunnels --command="'just scenario \"$DEPLOYMENT_NAME-$2-$3\" {{ city_name_node_targets }}'"
    }

    echo "Deploying..."

    echo 'will cite' | parallel --citation &> /dev/null || true

    export -f run_scenario
    parallel --tty --shuf \
        --joblog ./joblog \
        run_scenario {#} {1} {2} \
        ::: $(echo ${FOG_NODE_IMAGE_TAGS[@]}) \
        :::+ $(echo ${MARKET_IMAGE_TAGS[@]})

    for INDEX in $(seq 1 3); do
        parallel --tty --joblog ./joblog --retry-failed
    done

collect:
    mkdir -p metrics
    {{ RUN }} integration.py collect --address $INFLUX_ADDRESS

_collect:
    mkdir -p metrics
    {{ RUN }} integration.py collect

logs *FLAGS:
    {{ RUN }} integration.py logs {{ FLAGS }}

[private]
docker_enos name expe_dir cmd="bash":
    #!/usr/bin/env bash
    set -ex
    eval $(ssh-agent -s)
    rsync -rL {{ justfile_directory() }}/* {{ expe_dir }}

    echo "LOAD_NET: $LOAD_NETWORK_FILE"

    export REFRESH="false"
    if {{ DOCKER }} ps -a --format '{{{{.Names}}}}' | grep -wq "{{ name }}"; then
        export REFRESH="true"
        {{ DOCKER }} exec "{{ name }}" bash -c -E "export REFRESH=$REFRESH; {{ cmd }}"
    else
        {{ DOCKER }} run -it --rm \
            --privileged \
            --cap-add=ALL \
            --workdir /home/enos \
            -v /lib/modules:/lib/modules \
            -v {{ expe_dir }}:/home/enos \
            -v {{ justfile_directory() }}/metrics-arks:/home/enos/metrics-arks \
            -v {{ justfile_directory() }}/logs:/home/enos/logs \
            -v $LOAD_NETWORK_FILE:/home/enos/net:ro \
            -v ~/.python-grid5000.yaml:/root/.python-grid5000.yaml:ro \
            -v ~/.ssh/id_rsa:/root/.ssh/id_rsa:ro \
            -v ~/.ssh/id_rsa.pub:/root/.ssh/id_rsa.pub:ro \
            -v $(readlink -f $SSH_AUTH_SOCK):/ssh-agent \
            -v /etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt:ro \
            -e SSH_AUTH_SOCK=/ssh-agent \
            -e DEPLOYMENT_NAME="{{ name }}" \
            -e EXPE_LOAD_FILE=/home/enos/requests \
            -e LOAD_NETWORK_FILE=/home/enos/net \
            -e REFRESH="false" \
            --name "{{ name }}" \
            --cgroup-manager=cgroupfs \
            enos_deployment:latest bash -c -E "{{ cmd }}"
        fi

[private]
_inside_docker_scenearii +city_name_node_targets:
    #!/usr/bin/env bash
    echo "Generating requests for all subsequent experiments..."
    just scenarii {{ city_name_node_targets }}
    res=$?
    if [ $DEV_SLEEP_AFTER_CRASH = "true" ]; then
      if [ $res -ne 0 ]; then
        echo "DEV_SLEEP_AFTER_CRASH enabled, sleeping until infinity"
        sleep 9999999999
      fi
    fi
    just clean

[private]
docker_scenarii name expe_dir +city_name_node_targets:
    EXPE_SAVE_FILE={{ expe_dir }}/requests TARGET_NODE_NAMES="{{ city_name_node_targets }}" {{ RUN }} expe.py
    just docker_enos {{ name }} {{ expe_dir }} "just _inside_docker_scenearii {{ city_name_node_targets }}"

_docker_campaign name expe_dir:
    #!/usr/bin/env bash
    set -e
    cities=()
    for input in $({{ RUN }} integration.py iot-connections); do cities[${#cities[@]}]="'$input'"; done
    just docker_scenarii {{ name }} {{ expe_dir }} ${cities[@]}

single_campaign name dotenvfile_default dotenvfile_variation:
    #!/usr/bin/env bash
    set -ex
    expe_dir="/tmp/{{ name }}"
    mkdir -p $expe_dir

    cp {{ dotenvfile_default }} $expe_dir/.env
    cat {{ dotenvfile_variation }} >> $expe_dir/.env

    cp definitions.py $expe_dir/definitions.py
    mkdir -p metrics-arks logs

    today=$( date +%Y%m%d )
    number=0
    fname={{ name }}-$today.log
    while [ -e "$fname" ]; do
        printf -v fname '%s-%02d.log' "{{ name }}-$today" "$(( ++number ))"
    done

    echo "qsdklhjfjqhsjok: $LOAD_NETWORK_FILE"

    mkdir -p logs_campaign
    just _docker_campaign {{ name }} $expe_dir |& tee -a logs_campaign/$fname

[private]
build_required_images user skip_rebuild:
    #!/usr/bin/env bash
    set -e

    if [ "{{ skip_rebuild }}" != "true" ]; then
      read -ra tags <<<"$FOG_NODE_IMAGE_TAGS"
      read -ra market_tags <<<"$MARKET_IMAGE_TAGS"

      commands=(
          "cd $MANAGER_PATH && nix develop .#manager -c just ghcr {{ user }} $(echo ${tags[@]}) $(echo ${market_tags[@]})"
          "cd $IOT_EMULATION_PATH && nix develop .#iot_emulation -c just ghcr {{ user }}"
          "cd $FUNCTIONS_PATH && nix develop .#openfaas-functions -c just ghcr_all {{ user }}"
      )

      parallel --will-cite --halt-on-error 2 {1} 2> /dev/null ::: "${commands[@]}"
      echo -e "[{{ file_name(justfile_directory()) }}] all images uploaded \033[1;32mOK\033[0m"
    else
      echo -e "[{{ file_name(justfile_directory()) }}] all images upload \033[34mSKIPPING\033[0m"
    fi

_docker_campaign_in_env variation experiments_dotfile single_experiment_dotenvfile:
    #!/usr/bin/env bash
    set -ex

    function expe_command() {
        set -e
        export ii=$1

        sleep 1

        # Calculate current time in seconds
        current_time=$( date +%s )

        # Convert job duration to seconds
        IFS=":" read hours minutes seconds <<< "${DEPLOYMENT_WALLTIME}"
        job_duration_seconds=$(bc <<< "${hours}*3600 + ${minutes}*60 + ${seconds}")

        # Calculate threshold in seconds
        next_day=$(date -d ${DO_NOT_EXECUTE_IF_ENDING_AFTER} +"%Y%m%d")
        threshold_time=$(date -d "${next_day} ${DO_NOT_EXECUTE_IF_ENDING_AFTER_HOUR}" +%s)

        # Compare times and decide whether to execute the command
        if (( current_time + job_duration_seconds > threshold_time )); then
            >&2 echo "Job will finish after $DO_NOT_EXECUTE_IF_ENDING_AFTER_HOUR the '$DO_NOT_EXECUTE_IF_ENDING_AFTER' day. Command will not be executed."
            exit 127
        fi

        suffix=`echo "$ii""$current_time"`
        timeout --foreground ${job_duration_seconds} just single_campaign "{{ variation }}{{ single_experiment_dotenvfile }}_$suffix" {{ single_experiment_dotenvfile }} $ENV_VARIATION
    }

    # Import all the settings for multiple experiments and load the var as env vars
    set -o allexport
    source {{ experiments_dotfile }}
    set +o allexport

    if [ "$DEV" = "true" ]; then
        if [ "$DEV_NETWORK" = "true" ]; then
            export LOAD_NETWORK_FILE=$(mktemp)
        else
            export SAVE_NETWORK_FILE=$(mktemp)
            set -- $SIZE_MULTIPLIERS
            export SIZE_MULTIPLIER=$1
            set -- $MIN_NUMBER_VMS
            export MIN_NB_VMS=$1
            set -- $MAX_NUMBER_NODES
            export MAX_NB_NODES=$1

            {{ RUN }} definitions.py
            export LOAD_NETWORK_FILE=$SAVE_NETWORK_FILE

            unset SAVE_NETWORK_FILE
        fi
        just single_campaign "{{ variation }}{{ single_experiment_dotenvfile }}_DEV" {{ single_experiment_dotenvfile }}
        exit 0
    fi

    mkdir -p $JOB_DIR
    if [ $TYPE == "first_run" ]; then
        rm -rf $JOB_DIR/*
        parallel --will-cite \
            SAVE_NETWORK_FILE=$JOB_DIR/{1}-{4}.net \
            SIZE_MULTIPLIER={1} \
            MIN_NB_VMS={2} \
            MAX_NB_NODES={3} \
            {{ RUN }} definitions.py \
            ::: $SIZE_MULTIPLIERS \
            :::+ $MIN_NUMBER_VMS \
            :::+ $MAX_NUMBER_NODES \
            ::: $(seq 1 $NB_REPETITIONS)
    fi

    for i in $(seq 1 $NB_REPETITIONS); do
        args+=("")
    done

    export free_port=$(comm -23 <(seq 49152 65535 | sort) <(ss -Htan | awk '{print $4}' | cut -d':' -f2 | sort -u) | shuf | head -n 1)

    function encapsulated_expe_command() {
        set -ex
        export ENV_VARIATION=$1
        export LOAD_NETWORK_FILE=$3
        echo "Running the expe command with LOAD_NETWORK_FILE=$LOAD_NETWORK_FILE"
        expe_command $2
    }

    # Enable job control
    set -m
    export -f expe_command
    export -f encapsulated_expe_command

    set +e
    if [ ! -f $JOB_LOG ] && [ "$TYPE" != "first_run" ]; then
        >&2 echo "the action is TYPE=$TYPE; cannot run without JOB_LOG=$JOB_LOG existing"
        exit 1
    fi
    if [ $TYPE == "retry" ]; then
        (parallel --will-cite --retry-failed --joblog $JOB_LOG)
    else
        env_variations=`ls -a | grep ^.env.[0-9]*$`
        if [ "$DEV" = "true" ]; then
          env_variations=".env.dev"
        fi
        cmd=$(cat <<EOF
        --joblog $JOB_LOG \
        -j $NB_IN_PARALLEL \
        encapsulated_expe_command {1} {#} '$JOB_DIR/'{2}-{3}.net \
        ::: $env_variations
        ::: $SIZE_MULTIPLIERS \
        ::: $(seq 1 $NB_REPETITIONS)
    EOF
    )
        if [ $TYPE == "resume" ]; then
            parallel --will-cite --resume $cmd
        else
            rm $JOB_LOG || true
            parallel --will-cite $cmd
        fi
    fi

    echo "Here is the JOB_LOG:"
    cat $JOB_LOG

dry-experiment $CLUSTER SIZE_MULTIPLIERS:
    #!/usr/bin/env bash
    parallel --will-cite -k \
        export SIZE_MULTIPLIER={1} FILE='$(mktemp)' \
        ";" SAVE_NETWORK_FILE='$FILE' {{ RUN }} definitions.py ">" /dev/null \
        "&&" LOAD_NETWORK_FILE='$FILE' {{ RUN }} integration.py up --dry-run \
        ";" rm '$FILE' \
        ::: {{ SIZE_MULTIPLIERS }}

city experiments_dotfile=".experiments.env":
    #!/usr/bin/env bash
    set -o allexport
    source {{ experiments_dotfile }}
    {{ RUN }} master.py get-city

upload experiments_dotfile=".experiments.env" skip_vms="false":
    #!/usr/bin/env bash
    set -e
    set -o allexport
    source {{ experiments_dotfile }}
    city=$({{ RUN }} master.py get-city)

    push_vms(){
      if [ {{ skip_vms }} != "true" ]; then
        # Cannot build multiple VMs at the same time, so we do it sequential
        vm_path=$(nix build --extra-experimental-features nix-command --extra-experimental-features flakes .#enosvm --print-out-paths --no-link --quiet 2> /dev/null)/nixos.qcow2
        parallel ::: \
          "rsync -cazpq --inplace --stats --perms --chmod=u+rwx,g+rwx,o+rwx $vm_path $city.grid5000.fr:~/nixos.env.qcow2 \
          && echo -e '[{{ file_name(justfile_directory()) }}] master VM uploaded \033[32mOK\033[0m'" \
          "(cd iso; nix develop .#iso -c just upload $city > /dev/null 2>&1 \
          && echo -e '[{{ file_name(justfile_directory()) }}] fog_node VM uploaded \033[32mOK\033[0m')"
      else
        echo -e '[{{ file_name(justfile_directory()) }}] fog_node VM upload \033[34mSKIPPING\033[0m'
        echo -e '[{{ file_name(justfile_directory()) }}] master VM upload \033[34mSKIPPING\033[0m'
      fi
    }

    push_files(){
      parallel --will-cite rsync -cazpq --inplace --stats --perms --chmod=u+rwx,g+rwx,o+rwx {} $city.grid5000.fr:~/enosvm/{} 2> /dev/null \
          ::: *.py .env* {{ experiments_dotfile }} justfile pipelines/
      echo -e "[{{ file_name(justfile_directory()) }}] config files uploaded \033[32mOK\033[0m"
    }

    ssh $city.grid5000.fr mkdir -p enosvm 2> /dev/null

    parallel ::: \
      push_vms \
      push_files

    wait
    echo -e "[{{ file_name(justfile_directory()) }}] all files rsynced \033[1;32mOK\033[0m"

master_exec user name experiments_dotfile=".experiments.env" skip_vms="false" skip_rebuild="false":
    #!/usr/bin/env bash
    set -e
    set -o allexport
    source {{ experiments_dotfile }}
    set +o allexport

    city=$({{ RUN }} master.py get-city)
    username=$({{ RUN }} master.py get-username)
    echo "nfs:/export/home/$username" | tee iso/config/g5k.nfs.txt
    echo "ntp.$city.grid5000.fr" | tee iso/config/ntp-servers.txt

    parallel ::: \
      'just upload {{ experiments_dotfile }} {{ skip_vms }}' \
      'just build_required_images {{ user }} {{ skip_rebuild }}'

    {{ RUN }} master.py up --name {{ name }} --walltime $MASTER_WALLTIME --force
    {{ RUN }} master.py run-command

master_refresh user name experiments_dotfile=".experiments.env" skip_rebuild="false":
    #!/usr/bin/env bash
    set -e
    set -o allexport
    source {{ experiments_dotfile }}
    set +o allexport

    parallel ::: \
      'just upload {{ experiments_dotfile }} true' \
      'just build_required_images {{ user }} {{ skip_rebuild }}'
    {{ RUN }} master.py run-command-refresh

master_docker_campaign variation="valuation_rates" experiments_dotfile=".experiments.env" single_experiment_dotenvfile=".env":
    just _docker_campaign_in_env {{ variation }} {{ experiments_dotfile }} {{ single_experiment_dotenvfile }}

master_run:
    nix develop .#enosvm -c just _master_run

_master_run:
    #!/usr/bin/env bash
    set -e
    vm_path=$(nix build --extra-experimental-features nix-command --extra-experimental-features flakes .#enosvm --print-out-paths --no-link --quiet)/nixos.qcow2
    temp=nixos.qcow2
    cp $vm_path $temp
    chmod u+rwx $temp

    qemu-kvm \
        -cpu max \
        -name nixos \
        -m 4096 \
        -smp 4 \
        -drive cache=writeback,file="$temp",id=drive1,if=none,index=1,werror=report -device virtio-blk-pci,drive=drive1 \
        -net nic,netdev=giraff.master_vm,model=virtio -netdev user,id=giraff.master_vm,hostfwd=tcp::2222-:22 \
        -enable-kvm \
        -nographic&

    wait

master_ssh_in:
    nix develop .#enosvm -c sshpass -e ssh -t -oUserKnownHostsFile=/dev/null -oStrictHostKeyChecking=no root@127.0.0.1 -p 2222

get_metrics_back grep="" folder="metrics-arks" experiments_dotfile=".experiments.env":
    #!/usr/bin/env bash
    set -o allexport
    source {{ experiments_dotfile }}
    set +o allexport
    city=$({{ RUN }} master.py get-city)
    echo $city
    mkdir -p metrics-arks
    output1=$(ls ./metrics-arks | grep {{ grep }})
    ssh $city.grid5000.fr ls {{ folder }} | grep {{ grep }} | parallel --will-cite -j5 rsync -cazpq $city.grid5000.fr:~/{{ folder }}/{} ./metrics-arks/{}
    output2=$(ls ./metrics-arks | grep {{ grep }})
    diff_output=$(diff <(echo "$output1") <(echo "$output2"))
    echo -e "$diff_output"

[private]
test_expe experiments_dotfile=".experiments.env":
    #!/usr/bin/env bash
    set -o allexport
    source {{ experiments_dotfile }}
    set +o allexport
    export TARGET_NODE_NAMES=toto
    export TARGET_NODES=toto
    EXPE_SAVE_FILE=toto.txt python expe.py

[private]
test_definitions experiments_dotfile=".experiments.env":
    #!/usr/bin/env bash
    set -o allexport
    source {{ experiments_dotfile }}
    set +o allexport
    export SAVE_NETWORK_FILE=/dev/null
    python definitions.py
