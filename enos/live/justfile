set dotenv-load

RUN := env_var_or_default("RUN", "python")
DOCKER := env_var_or_default("DOCKER", "podman")
export MANAGER_PATH := "../../manager"
export IOT_EMULATION_PATH := "../../iot_emulation"
export FUNCTIONS_PATH := "../../openfaas-functions"

# Entrypoint, run deploy and then tunnels
_default:
    @just --list

init user *FLAGS:
    #!/usr/bin/env bash
    cat <<EOF > ~/.ssh/config
    Host !access.grid5000.fr *.grid5000.fr
    User {{user}}
    ProxyJump {{user}}@access.grid5000.fr
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
    ForwardAgent yes
    EOF

    {{RUN}} integration.py init --g5k_user={{user}} {{FLAGS}}

lab:
    nix run .#jupyterlab

labExport:
    nix run .#jupyterlabExport

# Open SSH tunnels
tunnels +FLAGS='':
    {{RUN}} integration.py tunnels {{FLAGS}}

# List end nodes of the Fog network
endpoints:
	{{RUN}} integration.py endpoints

_is_connected city_name_node_target market_port:
    #!/usr/bin/env bash
    market_ip=$({{RUN}} integration.py market-ip | grep address | sed 's/address: //')
    if [[ $(curl -s "$market_ip:{{market_port}}/api/fog" | jq '.[] | select(.tags[] == "{{city_name_node_target}}") | .id' | sed -nE 's/"(.*)"/\1/p') ]]; then
        echo "✅ {{city_name_node_target}}@$market_ip is connected"
    else
        echo "❌ {{city_name_node_target}}@$market_ip"
    fi

is_connected city_name_node_target market_local_port:
    watch --no-title --color --no-wrap just _is_connected {{city_name_node_target}} {{market_local_port}}

deploy fog_node_image market_image $DEPLOYMENT_NAME:
    {{RUN}} integration.py up --name="$DEPLOYMENT_NAME" --walltime "$DEPLOYMENT_WALLTIME" --force
    just _deploy {{fog_node_image}} {{market_image}}

refresh fog_node_image market_image $DEPLOYMENT_NAME:
    {{RUN}} integration.py restart
    just _deploy {{fog_node_image}} {{market_image}}

# Delete the Job on grid'5000 and local EnosLib files
clean:
    {{RUN}} integration.py clean || true
    rm -rf enos_* current cachedir __enos*   

# Refresh the container images hosted by k3s on all deployed nodes
_deploy fog_node_image market_image:
    #!/usr/bin/env bash
    set -e
    {{RUN}} integration.py network

    {{RUN}} integration.py iot-emulation&
    ({{RUN}} integration.py k3s-setup && \
        {{RUN}} integration.py k3s-deploy --fog_node_image={{fog_node_image}} --market_image={{market_image}})&
    
    wait

expe $IOT_IP $MARKET_IP $TARGET_NODES:
    {{RUN}} expe.py

scenario archive_name +city_name_node_targets:
    #!/usr/bin/env bash
    set -ex
    NODE_TARGETS=()
    MARKET_IP=$({{RUN}} integration.py market-ip | grep address | sed 's/address: //')
    for city in {{city_name_node_targets}}; do
        echo "Checking connection to $city"
        until [[ "$(just _is_connected $city $MARKET_LOCAL_PORT)" =~ "✅" ]]; do sleep 10 && echo -e "."; done
        NODE_TARGETS+=($(curl "$MARKET_IP:$MARKET_LOCAL_PORT/api/fog" | jq ".[] | select(.tags[] == \"$city\") | .id" | sed -nE 's/"(.*)"/\1/p'))
    done
    IOT_IP=$(just endpoints | sed -nE 's/Iot emulation IP -> (.*)/\1/p')
    printf -v joined '%s\t' "${NODE_TARGETS[@]}"
    export TARGET_NODE_NAMES="{{city_name_node_targets}}"
    
    just expe $IOT_IP $MARKET_IP "$joined"
    
    sleep $WAIT_TIME

    export COLLECT_ARCHIVE_NAME="{{archive_name}}"
    {{RUN}} integration.py collect

    if [ $DEV ]; then sleep 9999999999; fi

scenarii +city_name_node_targets:
    #!/usr/bin/env bash
    set -e

    # Function to generate a random number within a range
    get_random_number() {
    local min=$1
    local max=$2
    echo $((RANDOM % (max - min + 1) + min))
    }

    # Function to shuffle the array
    shuffle_array() {
    local array=("$@")
    local array_size=${#array[@]}

    for ((i = 0; i < array_size; i++)); do
        # Generate a random index
        local random_index=$(get_random_number 0 $((array_size - 1)))

        # Swap the current element with the element at the random index
        local temp=${array[i]}
        array[i]=${array[random_index]}
        array[random_index]=$temp
    done

    # Print the shuffled array
    echo "${array[@]}"
    }

    # Convert the space-separated list of fog node image tags to a Bash array
    TAGS=(${FOG_NODE_IMAGE_TAGS// / })

    TAGS=($(shuffle_array "${TAGS[@]}"))

    for INDEX in "${!TAGS[@]}"; do
        TAG="${TAGS[INDEX]}"
        echo "Using image $IMAGE_REGISTRY/$FOG_NODE_IMAGE_NAME:$TAG and $IMAGE_REGISTRY/$MARKET_IMAGE"
        if [ "$INDEX" -eq 0 ]; then
            just deploy "$IMAGE_REGISTRY/$FOG_NODE_IMAGE_NAME:$TAG" "$IMAGE_REGISTRY/$MARKET_IMAGE" $DEPLOYMENT_NAME
        else
            just refresh "$IMAGE_REGISTRY/$FOG_NODE_IMAGE_NAME:$TAG" "$IMAGE_REGISTRY/$MARKET_IMAGE" $DEPLOYMENT_NAME \
            || just deploy "$IMAGE_REGISTRY/$FOG_NODE_IMAGE_NAME:$TAG" "$IMAGE_REGISTRY/$MARKET_IMAGE" $DEPLOYMENT_NAME
        fi

        just tunnels --command="'just scenario \"$DEPLOYMENT_NAME-$TAG\" {{city_name_node_targets}}'"
    done

    just clean

collect:
    mkdir -p metrics
    {{RUN}} integration.py collect --address $INFLUX_ADDRESS

_collect:
    mkdir -p metrics
    {{RUN}} integration.py collect

logs *FLAGS:
    {{RUN}} integration.py logs {{FLAGS}}

vpn:
    #!/usr/bin/env bash
    set -e
    modprobe tun
    rm -rf grid5000-openvpn || true
    mkdir grid5000-openvpn
    unzip grid5000-openvpn.zip -d grid5000-openvpn
    cd grid5000-openvpn
    cat /root/.python-grid5000.yaml | grep 'password: ' | sed 's/password: //' > grid5000vpnpassword
    cat <<EOF >> Grid5000_VPN.ovpn
    script-security 2
    up /etc/openvpn/update-resolv-conf
    down /etc/openvpn/update-resolv-conf
    askpass "$PWD/grid5000vpnpassword"
    daemon
    EOF
    openvpn Grid5000_VPN.ovpn

sshconfig:
    #!/usr/bin/env bash
    set -e
    login=$(cat /root/.python-grid5000.yaml | grep 'username: ' | sed 's/username: //')

docker_enos name tempdir cmd="bash":
    #!/usr/bin/env bash
    set -e
    {{DOCKER}} run -it --rm \
        --privileged \
        --cap-add=ALL \
        --workdir /home/enos \
        -v /lib/modules:/lib/modules \
        -v {{justfile_directory()}}/integration.py:/home/enos/integration.py:ro \
        -v {{justfile_directory()}}/collect.py:/home/enos/collect.py:ro \
        -v {{tempdir}}/definitions.py:/home/enos/definitions.py:ro \
        -v {{justfile_directory()}}/expe.py:/home/enos/expe.py:ro \
        -v {{justfile_directory()}}/justfile:/home/enos/justfile:ro \
        -v {{justfile_directory()}}/metrics-arks:/home/enos/metrics-arks \
        -v {{justfile_directory()}}/logs:/home/enos/logs \
        -v {{tempdir}}/.env:/home/enos/.env:ro \
        -v {{tempdir}}/requests:/home/enos/requests:ro \
        -v $GRID5000OPENVPNZIP:/home/enos/grid5000-openvpn.zip:ro \
        -v ~/.python-grid5000.yaml:/root/.python-grid5000.yaml:ro \
        -v ~/.ssh/id_rsa:/root/.ssh/id_rsa:ro \
        -v ~/.ssh/id_rsa.pub:/root/.ssh/id_rsa.pub:ro \
        -v $(readlink -f $SSH_AUTH_SOCK):/ssh-agent \
        -e SSH_AUTH_SOCK=/ssh-agent \
        -e DEPLOYMENT_NAME="{{name}}" \
        -e EXPE_LOAD_FILE=/home/enos/requests \
        -e NB_FUNCTIONS_LOW_REQ_INTERVAL_LOW_LATENCY=$NB_FUNCTIONS_LOW_REQ_INTERVAL_LOW_LATENCY \
        -e NB_FUNCTIONS_HIGH_REQ_INTERVAL_LOW_LATENCY=$NB_FUNCTIONS_HIGH_REQ_INTERVAL_LOW_LATENCY \
        -e NB_FUNCTIONS_LOW_REQ_INTERVAL_REST_LATENCY=$NB_FUNCTIONS_LOW_REQ_INTERVAL_REST_LATENCY \
        -e NB_FUNCTIONS_HIGH_REQ_INTERVAL_REST_LATENCY=$NB_FUNCTIONS_HIGH_REQ_INTERVAL_REST_LATENCY \
        --name "{{name}}" \
        --cgroup-manager=cgroupfs \
        enos_deployment:latest bash -c -E "just sshconfig && {{cmd}}"

# --cgroup-manager=cgroupfs fix needs to be investigated because ... Linux ?!?

docker_scenarii name tempdir +city_name_node_targets:
    echo "Generating requests for all subsequent experiments..."
    EXPE_SAVE_FILE={{tempdir}}/requests TARGET_NODE_NAMES="{{city_name_node_targets}}" {{RUN}} expe.py
    just docker_enos {{name}} {{tempdir}} "(just vpn && just scenarii {{city_name_node_targets}}) ; just clean"

_docker_campain name tempdir:
    #!/usr/bin/env bash
    set -e
    cities=()
    for input in $({{RUN}} integration.py iot-connections); do cities[${#cities[@]}]="'$input'"; done
    just docker_scenarii {{name}} {{tempdir}} ${cities[@]}

single_campain suffix dotenvfile:
    #!/usr/bin/env bash
    set -e
    tempdir=$(mktemp -d)
    cp {{dotenvfile}} $tempdir/.env
    cp definitions.py $tempdir/definitions.py
    mkdir -p metrics-arks logs
    # Build image before calling it in parrallel as it confuses podman

    today=$( date +%Y%m%d )
    number=0
    fname={{suffix}}-$today.log
    while [ -e "$fname" ]; do
        printf -v fname '%s-%02d.log' "{{suffix}}-$today" "$(( ++number ))"
    done

    mkdir -p logs_campaign

    # command receives its input from stdin.
    # command sends its output to stdout.
    exec 3>&1
    stderr="$(just _docker_campain {{suffix}} $tempdir </dev/stdin 2>&1 1>&3)"
    exitcode="${?}"
    echo "$stderr" | tee logs_campaign/$fname
    exit ${exitcode}

build_required_images user:
    #!/usr/bin/env bash
    set -e

    for (( i=0; i<${#FOG_NODE_IMAGE_TAGS[@]}; i++ )); do
        FOG_NODE_IMAGE_TAGS[$i]="fog_node_${FOG_NODE_IMAGE_TAGS[$i]}"
    done

    commands=(
        "cd $MANAGER_PATH && nix develop -c just ghcr {{user}} ${FOG_NODE_IMAGE_TAGS[@]}"
        "cd $IOT_EMULATION_PATH && nix develop -c just ghcr {{user}}"
        "cd $FUNCTIONS_PATH && nix develop -c just faas"
    )

    parallel --lb  ::: "${commands[@]}"

docker_campain user variation="valuation_rates" experiments_dotfile=".env.experiments"  single_experiment_dotenvfile=".env": (build_required_images user)
    nix develop --extra-experimental-features "nix-command flakes" -c just _docker_campain_in_env {{user}} {{variation}} {{experiments_dotfile}} {{single_experiment_dotenvfile}}

_docker_campain_in_env user variation experiments_dotfile single_experiment_dotenvfile:
    #!/usr/bin/env bash
    set -e
    # image_path=$(nix build --extra-experimental-features nix-command --extra-experimental-features flakes .#docker --print-out-paths --no-link --quiet)
    # {{DOCKER}} load < $image_path
    
    function expe_command() {
        set -e
        export ii=$1

        # Calculate current time in seconds
        current_time=$(date +%s)

        # Convert job duration to seconds
        IFS=":" read hours minutes seconds <<< "${DEPLOYMENT_WALLTIME}"
        job_duration_seconds=$(bc <<< "${hours}*3600 + ${minutes}*60 + ${seconds}")

        # Calculate threshold in seconds
        next_day=$(date -d ${DO_NOT_EXECUTE_IF_ENDING_AFTER} +"%Y%m%d")
        threshold_time=$(date -d "${next_day} ${DO_NOT_EXECUTE_IF_ENDING_AFTER_HOUR}" +%s)

        # Compare times and decide whether to execute the command
        if (( current_time + job_duration_seconds > threshold_time )); then
            echo "Job will finish after $DO_NOT_EXECUTE_IF_ENDING_AFTER_HOUR the '$DO_NOT_EXECUTE_IF_ENDING_AFTER' day. Command will not be executed."
            exit 127
        fi

        just single_campain "{{variation}}{{single_experiment_dotenvfile}}_$ii" {{single_experiment_dotenvfile}}
    }

    if [ $DEV ]; then 
        expe_command "dev" 
        exit 0
    fi


    # Import all the settings for multiple experiments and load the var as env vars
    set -o allexport
    source {{experiments_dotfile}}
    set +o allexport

    for i in $(seq 1 $NB_REPETITIONS); do
        args+=("")
    done

    export free_port=$(comm -23 <(seq 49152 65535 | sort) <(ss -Htan | awk '{print $4}' | cut -d':' -f2 | sort -u) | shuf | head -n 1)

    function encapsulated_expe_command_mprocs() {
        set -e
        JOB_TERMINATION=$(mktemp)
        ctl=$(cat <<EOF
    export NB_FUNCTIONS_LOW_REQ_INTERVAL_LOW_LATENCY=$1
    ; export NB_FUNCTIONS_HIGH_REQ_INTERVAL_LOW_LATENCY=$2
    ; export NB_FUNCTIONS_LOW_REQ_INTERVAL_REST_LATENCY=$3
    ; export NB_FUNCTIONS_HIGH_REQ_INTERVAL_REST_LATENCY=$4
    ; bash -c "set -e; expe_command $5"
    ; exit_status=\$?
    ; exit \$(echo \$exit_status | tee $JOB_TERMINATION)
    EOF
        )
        mprocs --server 0:$free_port --ctl "{c: add-proc, cmd: '$ctl'}"
        until [ $(wc -l < "${JOB_TERMINATION}") -ne 0 ]; do sleep 5; done
        job_status=$(cat $JOB_TERMINATION)
        rm $JOB_TERMINATION
        exit $job_status
    }

    # Enable job control
    set -m
    export -f expe_command
    export -f encapsulated_expe_command_mprocs
    mprocs --server 0:$free_port&
    
    set +e
    if [ ! -f $JOB_LOG ] && [ $TYPE != "first_run" ]; then
        echo "the action is TYPE=$TYPE; cannot run without JOB_LOG=$JOB_LOG existing"
        exit 1
    fi
    if [ $TYPE == "retry" ]; then
        (parallel --retry-failed --joblog $JOB_LOG)&
    else
        cmd=$(cat <<EOF
        --joblog $JOB_LOG \
        -j $NB_IN_PARALLEL \
        encapsulated_expe_command_mprocs {1} {2} {3} {4} {#} \
        ::: $NB_FUNCTIONS_LOW_REQ_INTERVAL_LOW_LATENCY \
        :::+ $NB_FUNCTIONS_HIGH_REQ_INTERVAL_LOW_LATENCY \
        ::: $NB_FUNCTIONS_LOW_REQ_INTERVAL_REST_LATENCY \
        :::+ $NB_FUNCTIONS_HIGH_REQ_INTERVAL_REST_LATENCY \
        ::: $(seq 1 $NB_REPETITIONS)
    EOF
    )
        if [ $TYPE == "resume" ]; then
            parallel --resume $cmd&
        else
            rm $JOB_LOG || .
            parallel $cmd&
        fi
    fi

    fg %1
    fg %2
    
    echo "Here is the JOB_LOG:"
    cat $JOB_LOG
   
dev:
    nix develop --extra-experimental-features nix-command --extra-experimental-features flakes

sim filename="sim" nb_iter="1" $methods="" $pricing="" $RANDOM_SEED_INIT="" NB_FUNCTIONS_LOW_REQ_INTERVAL_LOW_LATENCY="5" NB_FUNCTIONS_HIGH_REQ_INTERVAL_LOW_LATENCY="5" NB_FUNCTIONS_LOW_REQ_INTERVAL_REST_LATENCY="100" NB_FUNCTIONS_HIGH_REQ_INTERVAL_REST_LATENCY="100" SIZE_MULTIPLIERS="1":
    #!/usr/bin/env bash
    export cities=()
    export pricing=${pricing:=$({{RUN}} simulator.py 2>/dev/null | sed -n 's/PRICING_STRATEGY not in \[\(.*\)\].*/\1/p')}
    export methods=${methods:=$({{RUN}} simulator.py 2>/dev/null | sed -n 's/PLACEMENT_STRATEGY not in \[\(.*\)\].*/\1/p')}
    export CSV_OUT_DIR=$(mktemp -d)

    parallel SAVE_NETWORK_FILE=$CSV_OUT_DIR/{1}-{2}.net SIZE_MULTIPLIER={1} {{RUN}} definitions.py > /dev/null ::: {{SIZE_MULTIPLIERS}} ::: $(seq 1 {{nb_iter}})

    time parallel --keep-order  \
        --memsuspend 3G \
        JOB_INDEX={#} \
        NB_FUNCTIONS_LOW_REQ_INTERVAL_LOW_LATENCY={1} \
        NB_FUNCTIONS_HIGH_REQ_INTERVAL_LOW_LATENCY={2} \
        NB_FUNCTIONS_LOW_REQ_INTERVAL_REST_LATENCY={3} \
        NB_FUNCTIONS_HIGH_REQ_INTERVAL_REST_LATENCY={4} \
        RANDOM_SEED='$(shuf -i 0-2560000 -n 1)' \
        just _sim $CSV_OUT_DIR/{5}-{6}.net \
        ::: {{NB_FUNCTIONS_LOW_REQ_INTERVAL_LOW_LATENCY}} \
        :::+ {{NB_FUNCTIONS_HIGH_REQ_INTERVAL_LOW_LATENCY}} \
        ::: {{NB_FUNCTIONS_LOW_REQ_INTERVAL_REST_LATENCY}} \
        :::+ {{NB_FUNCTIONS_HIGH_REQ_INTERVAL_REST_LATENCY}} \
        ::: {{SIZE_MULTIPLIERS}} \
        ::: $(seq 1 {{nb_iter}})

    rm ./{{filename}}.data.csv || .
    touch ./{{filename}}.data.csv
    readarray -d '' entries < <(printf '%s\0' $CSV_OUT_DIR/*.data.csv | sort -zV)
    for file in "${entries[@]}"; do
        cat $file >> ./{{filename}}.data.csv
    done
    rm ./{{filename}}.levels.csv || .
    touch ./{{filename}}.levels.csv
    readarray -d '' entries < <(printf '%s\0' $CSV_OUT_DIR/*.levels.csv | sort -zV)
    for file in "${entries[@]}"; do
        cat $file >> ./{{filename}}.levels.csv
    done

    rm -r $CSV_OUT_DIR

_sim $LOAD_NETWORK_FILE:
    #!/usr/bin/env bash
    export EXPE_SAVE_FILE=$(mktemp)

    inputs=$({{RUN}} integration.py iot-connections)
    for input in $inputs; do cities[${#cities[@]}]="'$input'"; done
    export TARGET_NODE_NAMES="${cities[@]}"
    {{RUN}} expe.py >&2
    parallel --memsuspend 3G \
        PLACEMENT_STRATEGY={1} \
        PRICING_STRATEGY={2} \
        JOB_INDEX=$JOB_INDEX{#} \
        JOB_ID=$JOB_INDEX-{#} \
        {{RUN}} simulator.py \
        ::: $methods \
        ::: $pricing 
    
    rm $EXPE_SAVE_FILE >&2

simple_sim $PLACEMENT_STRATEGY $PRICING_STRATEGY:
    #!/usr/bin/env bash
    set -e
    export SIZE_MULTIPLIER=1
    export cities=()
    inputs=$(SAVE_NETWORK_FILE=./network {{RUN}} integration.py iot-connections)
    for input in $inputs; do cities[${#cities[@]}]="'$input'"; done
    export EXPE_SAVE_FILE=./requests 
    export TARGET_NODE_NAMES="${cities[@]}"
    export RANDOM_SEED=42
    export LOAD_NETWORK_FILE=./network
    [[ -f $EXPE_SAVE_FILE ]] || {{RUN}} expe.py
    {{RUN}} simulator.py > sim.simple.dump.csv